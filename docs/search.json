[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anoop Singh",
    "section": "",
    "text": "Education\n\nMS Business Analytics | Rady School of Management | 06/2025\n\nBS Business Economics | UCSD | 06/2024\n\n\n\n\nWork History\n\nBusiness Analyst | Thermo Fisher Scientific | 03/2025 – Present\n\nTeaching Assistant | Customer Analytics | 09/2024 – Present\n\nProject Manager | Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\nFinance Intern | Bonduelle Fresh Americas | 06/2023 – 08/2023\n\nBusiness Development Intern | OmniSync Inc | 06/2022 – 08/2022\n\n\n\n\nLeadership\n\nVice President | Triton Consulting Group | 2023 – 2024\n\nProject Manager | Triton Consulting Group | 2022 – 2023\n\nMember | Rady Data Analytics Club | 2024 – 2025\n\n\n\n\nInterests & Tools\n\nLanguages & Tools: Python, SQL, R, Tableau, Power BI, PySpark, Hadoop, Databricks, AWS, Snowflake\n\nFun Stuff: Pickleball, Snowboarding, Hiking, Baking, Board Games, Machine Learning, Big Data"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Anoop Singh",
    "section": "",
    "text": "MS Business Analytics | Rady School of Management | 06/2025\n\nBS Business Economics | UCSD | 06/2024"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Anoop Singh",
    "section": "",
    "text": "Business Analyst | Thermo Fisher Scientific | 03/2025 – Present\n\nTeaching Assistant | Customer Analytics | 09/2024 – Present\n\nProject Manager | Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\nFinance Intern | Bonduelle Fresh Americas | 06/2023 – 08/2023\n\nBusiness Development Intern | OmniSync Inc | 06/2022 – 08/2022"
  },
  {
    "objectID": "index.html#leadership",
    "href": "index.html#leadership",
    "title": "Anoop Singh",
    "section": "",
    "text": "Vice President | Triton Consulting Group | 2023 – 2024\n\nProject Manager | Triton Consulting Group | 2022 – 2023\n\nMember | Rady Data Analytics Club | 2024 – 2025"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/index.html#sub-header",
    "href": "projects/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download Resume \n\n\nAnoop Singh\n\n\nEducation\n\n\n\nMaster of Science in Business Analytics, Rady School of Management, University of California, San Diego, CA | 06/2025 | GPA: 3.9/4.0\n\n\n\nRelevant Courses: Unstructured Data, SQL and ETL, Collecting and Analyzing Large Data, Customer Analytics\n\n\n\nBachelor of Science in Business Economics, University of California, San Diego, CA | 06/2024 | GPA: 3.5/4.0\n\n\n\n\nExperience\n\n\n\nSupply Chain Data Analyst, Thermo Fisher Scientific, San Diego, CA | 03/2025 – Present\n\n\n\nDeveloped a machine learning model in Databricks to predict supply chain risk, improving procurement efficiency and enabling proactive decision-making.\n\n\nBuilt an end-to-end data pipeline integrating 5 years of supply chain data into a centralized data lake, enhancing analytics for waste reduction.\n\n\n\nProject Manager, Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\n\n\nImplemented a solar power generator system with digital monitoring, reducing electricity costs by 80% in a rural community.\n\n\nLed market research and feasibility analysis for producing/distributing solar lanterns to promote economic self-sufficiency.\n\n\n\nFinance Intern, Bonduelle Fresh Americas, Los Angeles, CA | 06/2023 – 08/2023\n\n\n\nCreated a Tableau performance analysis tool to cut promotional inefficiencies, saving $15,000 annually.\n\n\nBuilt a regression model for cost prediction, enabling better contract management and supply chain decisions.\n\n\n\nBusiness Development Intern, OmniSync Inc, San Diego, CA | 06/2022 – 08/2022\n\n\n\nSupported 100+ startups in securing SBIR/STTR grants through compliance and proposal support.\n\n\nAutomated engagement workflows in Python, improving client retention and outreach using database insights.\n\n\n\n\n\nProjects\n\n\n\nFood Insecurity Dashboard, Business Intelligence Systems, UCSD | 09/2024 – 12/2024\n\n\n\nBuilt an interactive Tableau dashboard visualizing food insecurity in 84,000+ U.S. census tracts.\n\n\nCleaned over 1M records using SQL and Tableau Prep, delivering insights to nonprofits and advocacy groups.\n\n\n\nAnime Recommender System, Web Mining & Recommender Systems, UCSD | 09/2024 – 12/2024\n\n\n\nBuilt a personalized recommender model using 7.8M user-anime interactions; achieved RMSE of 1.56.\n\n\nPerformed EDA on 12,000+ titles to improve input quality and model accuracy.\n\n\n\n\n\nProfessional Affiliations & Leadership\n\n\n\nTeaching Assistant, Customer Analytics, UCSD | 09/2024 – Present\n\n\n\nLed instruction for 300+ students in segmentation techniques and R-based statistical modeling.\n\n\n\nVP of Internal Development, Triton Consulting Group, UCSD | 06/2023 – 06/2024\n\n\n\nDeveloped R-based customer segmentation and marketing strategy that increased boutique revenue by 10%.\n\n\nAutomated data governance workflows in Python, cutting cleanup time from hours to minutes.\n\n\n\n\n\nSkills & Interests\n\n\n\nProgramming & Analytics: Python, SQL, R, PySpark, Spark, ETL, A/B Testing, Tableau, Power BI, Excel\n\n\nCloud & Big Data Tools: AWS (EMR, SageMaker), Azure, Hadoop, Snowflake, Databricks, GitHub, CRM\n\n\nInterests: Pickleball, Snowboarding, Hiking, Weightlifting, Board Games, Baking, Machine Learning, Big Data"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nAnoop Singh\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nAnoop Singh\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Detection and Automated Fact-Checking on the LIAR Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#interests-tools",
    "href": "index.html#interests-tools",
    "title": "Anoop Singh",
    "section": "",
    "text": "Languages & Tools: Python, SQL, R, Tableau, Power BI, PySpark, Hadoop, Databricks, AWS, Snowflake\n\nFun Stuff: Pickleball, Snowboarding, Hiking, Baking, Board Games, Machine Learning, Big Data"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html",
    "href": "projects/project1/Liar_dataset_final.html",
    "title": "Fake News Classification with NLP",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport re\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.impute import SimpleImputer"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#load-and-preprocess-data",
    "href": "projects/project1/Liar_dataset_final.html#load-and-preprocess-data",
    "title": "Fake News Classification with NLP",
    "section": "Load and Preprocess Data",
    "text": "Load and Preprocess Data\n\n\n# Load dataset\ntrain_df = pd.read_csv(\"liar_dataset/train.tsv\", delimiter=\"\\t\", header=None)\ntest_df = pd.read_csv(\"liar_dataset/test.tsv\", delimiter=\"\\t\", header=None)\n\n# Define column names\ncolumn_names = [\n    \"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\",\n    \"State\", \"Party\", \"Barely_True_Count\", \"False_Count\", \"Half_True_Count\",\n    \"Mostly_True_Count\", \"Pants_On_Fire_Count\", \"Context\"\n]\ntrain_df.columns = column_names\ntest_df.columns = column_names\n\n# Handle NaNs\nnumeric_features = [\"Barely_True_Count\", \"False_Count\", \"Half_True_Count\", \"Mostly_True_Count\", \"Pants_On_Fire_Count\"]\ntrain_df = train_df.dropna(subset=numeric_features)\ntest_df = test_df.dropna(subset=numeric_features)\n\ncategorical_features = [\"Speaker\", \"Speaker_Job\", \"State\", \"Party\", \"Subject\", \"Context\"]\ntrain_df[categorical_features] = train_df[categorical_features].fillna(\"Unknown\")\ntest_df[categorical_features] = test_df[categorical_features].fillna(\"Unknown\")"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#feature-engineering",
    "href": "projects/project1/Liar_dataset_final.html#feature-engineering",
    "title": "Fake News Classification with NLP",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Preprocessing function for statements\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n    return text\n\ntrain_df[\"Processed_Statement\"] = train_df[\"Statement\"].apply(preprocess_text)\ntest_df[\"Processed_Statement\"] = test_df[\"Statement\"].apply(preprocess_text)\n\n# Simplify labels\nlabel_mapping = {\n    \"true\": \"true\", \"mostly-true\": \"true\",\n    \"half-true\": \"somewhat-true\", \"barely-true\": \"somewhat-true\",\n    \"false\": \"false\", \"pants-fire\": \"false\"\n}\ntrain_df[\"Simplified_Label\"] = train_df[\"Label\"].map(label_mapping)\ntest_df[\"Simplified_Label\"] = test_df[\"Label\"].map(label_mapping)"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#exploratory-data-analysis-eda",
    "href": "projects/project1/Liar_dataset_final.html#exploratory-data-analysis-eda",
    "title": "Fake News Classification with NLP",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n# Plot label distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Label\", order=train_df[\"Label\"].value_counts().index)\nplt.title(\"Distribution of Labels in LIAR Dataset\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Statement length column\ntrain_df[\"Statement_Length\"] = train_df[\"Statement\"].apply(lambda x: len(str(x).split()))\n\n# Color mapping for truthfulness\ntruthfulness_colors = {\"false\": \"red\", \"somewhat-true\": \"orange\", \"true\": \"blue\"}\n\n# Label Distribution Analysis\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\nplt.title(\"Distribution of Simplified Truthfulness Labels\")\nplt.xlabel(\"Truthfulness Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.show()\n\n/tmp/ipykernel_15365/4162792726.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\n\n\n\n\n\n\n\n\n\n\n# Speaker Analysis: Most Frequent Speakers\ntop_speakers = train_df[\"Speaker\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\nplt.title(\"Top 10 Most Frequent Speakers\")\nplt.xlabel(\"Speaker\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n/tmp/ipykernel_15365/1034616230.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n# Party Affiliation vs. Truthfulness\nparty_truth_table = pd.crosstab(train_df[\"Party\"], train_df[\"Simplified_Label\"], normalize=\"index\")\nparty_truth_table.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=[truthfulness_colors[label] for label in party_truth_table.columns])\nplt.title(\"Truthfulness Distribution by Political Party\")\nplt.xlabel(\"Political Party\")\nplt.ylabel(\"Proportion of Statements\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Subject Analysis\ntop_subjects = train_df[\"Subject\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Subjects in the Dataset\")\nplt.xlabel(\"Subject\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n/tmp/ipykernel_15365/3302470076.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n# Word Clouds\ntrue_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"true\"][\"Subject\"].dropna())\nfalse_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"false\"][\"Subject\"].dropna())\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nwordcloud_true = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Blues\").generate(true_subjects)\naxes[0].imshow(wordcloud_true, interpolation=\"bilinear\")\naxes[0].set_title(\"Subjects in True Statements\", color=\"blue\")\naxes[0].axis(\"off\")\n\nwordcloud_false = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Reds\").generate(false_subjects)\naxes[1].imshow(wordcloud_false, interpolation=\"bilinear\")\naxes[1].set_title(\"Subjects in False Statements\", color=\"red\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# State-Level Trends (False Statements) - Removing \"Unknown\"\n\n# Count false statements per state and remove \"Unknown\"\nfalse_statements_by_state = train_df[train_df[\"Simplified_Label\"] == \"false\"][\"State\"].value_counts()\nfalse_statements_by_state = false_statements_by_state[false_statements_by_state.index != \"Unknown\"].head(15)\n\n# Plot updated bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(x=false_statements_by_state.index, y=false_statements_by_state.values, color=\"red\")\nplt.title(\"Top 15 States with Most False Statements (Excluding Unknown)\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of False Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Job Title Trends - Removing \"Unknown\"\n\n# Remove \"Unknown\" job titles\ntop_jobs = train_df[train_df[\"Speaker_Job\"] != \"Unknown\"][\"Speaker_Job\"].value_counts().head(10)\n\n# Plot updated bar chart for top job titles\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Speaker Job Titles (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n/tmp/ipykernel_15365/2681968054.py:8: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n# Truthfulness breakdown by job title\njob_truth_table = pd.crosstab(train_df[\"Speaker_Job\"], train_df[\"Simplified_Label\"], normalize=\"index\")\njob_truth_table = job_truth_table.loc[top_jobs.index]  # Filter only top 10 jobs\n\n# Stacked bar chart with color coding\njob_truth_table.plot(kind=\"bar\", stacked=True, figsize=(12, 6), color=[truthfulness_colors[label] for label in job_truth_table.columns])\nplt.title(\"Truthfulness Breakdown by Job Title (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Proportion of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.show()"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#logistic-regression-models",
    "href": "projects/project1/Liar_dataset_final.html#logistic-regression-models",
    "title": "Fake News Classification with NLP",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df[\"Simplified_Label\"])\ny_test = label_encoder.transform(test_df[\"Simplified_Label\"])\n\n# Feature Extraction: TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), sublinear_tf=True)\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nparam_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\nlog_reg.fit(X_train_tfidf, y_train)\ny_pred = log_reg.predict(X_test_tfidf)\nprint(\"Baseline Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBaseline Logistic Regression Accuracy: 0.43567482241515393\n              precision    recall  f1-score   support\n\n           0       0.39      0.30      0.34       341\n           1       0.44      0.45      0.44       477\n           2       0.46      0.52      0.49       449\n\n    accuracy                           0.44      1267\n   macro avg       0.43      0.42      0.42      1267\nweighted avg       0.43      0.44      0.43      1267\n\n\n\n\n# Feature Extraction: Categorical Encoding\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\nX_train_categorical = one_hot_encoder.fit_transform(train_df[categorical_features])\nX_test_categorical = one_hot_encoder.transform(test_df[categorical_features])\n\n# Feature Extraction: Numerical Scaling\nscaler = StandardScaler()\nX_train_numeric = scaler.fit_transform(train_df[numeric_features])\nX_test_numeric = scaler.transform(test_df[numeric_features])\n\n# Combine Features\nX_train_combined = hstack([X_train_tfidf, X_train_categorical, X_train_numeric])\nX_test_combined = hstack([X_test_tfidf, X_test_categorical, X_test_numeric])\n\n\n\n# Logistic Regression Model Training\nlog_param_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), log_param_grid, cv=5)\nlog_reg.fit(X_train_combined, y_train)\n\n# Predictions\ny_pred = log_reg.predict(X_test_combined)\n\n# Evaluation\nprint(\"Best Logistic Regression Parameters:\", log_reg.best_params_)\nprint(\"Logistic Regression Accuracy with Added Metadata:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBest Logistic Regression Parameters: {'C': 0.1}\nLogistic Regression Accuracy with Added Metadata: 0.468034727703236\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.48      0.60      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267\n\n\n\n\n# Logistic Regression with TF-IDF (n-grams)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), sublinear_tf=True)\nX_train_tfidf_ngram = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf_ngram = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nX_train_combined_ngram = hstack([X_train_tfidf_ngram, X_train_categorical, X_train_numeric])\nX_test_combined_ngram = hstack([X_test_tfidf_ngram, X_test_categorical, X_test_numeric])\n\nlog_reg.fit(X_train_combined_ngram, y_train)\ny_pred_log_ngram = log_reg.predict(X_test_combined_ngram)\nprint(\"Logistic Regression with TF-IDF (n-grams) Accuracy:\", accuracy_score(y_test, y_pred_log_ngram))\nprint(classification_report(y_test, y_pred_log_ngram))\n\nLogistic Regression with TF-IDF (n-grams) Accuracy: 0.46724546172059983\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.47      0.59      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#support-vector-machine",
    "href": "projects/project1/Liar_dataset_final.html#support-vector-machine",
    "title": "Fake News Classification with NLP",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\n\n# Grid Search for SVM\nsvm_param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\nsvm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\nsvm_grid_search.fit(X_train_combined, y_train)\nbest_svm_params = svm_grid_search.best_params_\nprint(\"Best SVM Parameters:\", best_svm_params)\n\nBest SVM Parameters: {'C': 0.1, 'kernel': 'linear'}\n\n\n\n# Train SVM Model\nsvm_model = SVC(kernel=\"linear\", C=1.0)\nsvm_model.fit(X_train_combined, y_train)\ny_pred_svm = svm_model.predict(X_test_combined)\nprint(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm))\n\nSVM Accuracy: 0.43962115232833465\n              precision    recall  f1-score   support\n\n           0       0.41      0.39      0.40       341\n           1       0.45      0.41      0.43       477\n           2       0.45      0.51      0.48       449\n\n    accuracy                           0.44      1267\n   macro avg       0.44      0.44      0.44      1267\nweighted avg       0.44      0.44      0.44      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#random-forest",
    "href": "projects/project1/Liar_dataset_final.html#random-forest",
    "title": "Fake News Classification with NLP",
    "section": "Random Forest",
    "text": "Random Forest\n\n\n# Define hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [200, 500],\n    \"max_depth\": [10, 15, None],\n    \"min_samples_split\": [2, 5],\n    \"min_samples_leaf\": [1, 2]\n}\n\n# Perform Grid Search with 5-fold cross-validation\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, n_jobs=-1, verbose=1)\nrf_grid_search.fit(X_train_combined, y_train)\n\n# Get the best parameters and the best model\nbest_rf_params = rf_grid_search.best_params_\nbest_rf_model = rf_grid_search.best_estimator_\n\n# Evaluate the best Random Forest model\ny_pred_rf_best = best_rf_model.predict(X_test_combined)\nrf_best_accuracy = accuracy_score(y_test, y_pred_rf_best)\nrf_best_report = classification_report(y_test, y_pred_rf_best)\n\nprint(\"Random Forest Accuracy:\", rf_best_accuracy)\nprint(\"Classification Report:\\n\", rf_best_report)\n\n\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n\n\n/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nRandom Forest Accuracy: 0.5706393054459353\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.64      0.45      0.52       341\n           1       0.53      0.60      0.56       477\n           2       0.58      0.64      0.61       449\n\n    accuracy                           0.57      1267\n   macro avg       0.58      0.56      0.56      1267\nweighted avg       0.58      0.57      0.57      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html",
    "href": "projects/project1/Liar_dataset_final_full.html",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "",
    "text": "In an era of rapidly spreading misinformation, automated fact‐checking has become a critical tool for evaluating the veracity of public statements. This project leverages the LIAR dataset, a widely recognized benchmark for fake news detection, to build models that classify political statements into veracity categories. We explore a range of natural language processing (NLP) machine learning approaches. We hypothesize that integrating contextual metadata with textual features enhances the accuracy of automated fake news detection, outperforming models relying solely on textual information. Our experiments reveal that baseline classifiers such as Logistic Regression can perform relatively well and are improved when more advanced machine learning methods such as Random Forests are implemented. Overall, our findings underscore the trade-offs between model complexity, training data volume, and computational constraints in the quest for accurate automated fact-checking.\nimport pandas as pd\nimport numpy as np\nimport re\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.impute import SimpleImputer"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#load-and-preprocess-data",
    "href": "projects/project1/Liar_dataset_final_full.html#load-and-preprocess-data",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Load and Preprocess Data",
    "text": "Load and Preprocess Data\n\n\n# Load dataset\ntrain_df = pd.read_csv(\"liar_dataset/train.tsv\", delimiter=\"\\t\", header=None)\ntest_df = pd.read_csv(\"liar_dataset/test.tsv\", delimiter=\"\\t\", header=None)\n\n# Define column names\ncolumn_names = [\n    \"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\",\n    \"State\", \"Party\", \"Barely_True_Count\", \"False_Count\", \"Half_True_Count\",\n    \"Mostly_True_Count\", \"Pants_On_Fire_Count\", \"Context\"\n]\ntrain_df.columns = column_names\ntest_df.columns = column_names\n\n# Handle NaNs\nnumeric_features = [\"Barely_True_Count\", \"False_Count\", \"Half_True_Count\", \"Mostly_True_Count\", \"Pants_On_Fire_Count\"]\ntrain_df = train_df.dropna(subset=numeric_features)\ntest_df = test_df.dropna(subset=numeric_features)\n\ncategorical_features = [\"Speaker\", \"Speaker_Job\", \"State\", \"Party\", \"Subject\", \"Context\"]\ntrain_df[categorical_features] = train_df[categorical_features].fillna(\"Unknown\")\ntest_df[categorical_features] = test_df[categorical_features].fillna(\"Unknown\")"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#feature-engineering",
    "href": "projects/project1/Liar_dataset_final_full.html#feature-engineering",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Preprocessing function for statements\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n    return text\n\ntrain_df[\"Processed_Statement\"] = train_df[\"Statement\"].apply(preprocess_text)\ntest_df[\"Processed_Statement\"] = test_df[\"Statement\"].apply(preprocess_text)\n\n# Simplify labels\nlabel_mapping = {\n    \"true\": \"true\", \"mostly-true\": \"true\",\n    \"half-true\": \"somewhat-true\", \"barely-true\": \"somewhat-true\",\n    \"false\": \"false\", \"pants-fire\": \"false\"\n}\ntrain_df[\"Simplified_Label\"] = train_df[\"Label\"].map(label_mapping)\ntest_df[\"Simplified_Label\"] = test_df[\"Label\"].map(label_mapping)\n\n\nText Cleaning\nWe clean the statement text by removing special characters and lowercasing all letters. This normalizes text and improves model generalization."
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#exploratory-data-analysis-eda",
    "href": "projects/project1/Liar_dataset_final_full.html#exploratory-data-analysis-eda",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n# Plot label distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Label\", order=train_df[\"Label\"].value_counts().index)\nplt.title(\"Distribution of Labels in LIAR Dataset\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Statement length column\ntrain_df[\"Statement_Length\"] = train_df[\"Statement\"].apply(lambda x: len(str(x).split()))\n\n# Color mapping for truthfulness\ntruthfulness_colors = {\"false\": \"red\", \"somewhat-true\": \"orange\", \"true\": \"blue\"}\n\n# Label Distribution Analysis\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\nplt.title(\"Distribution of Simplified Truthfulness Labels\")\nplt.xlabel(\"Truthfulness Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Analysis: Most Frequent Speakers\ntop_speakers = train_df[\"Speaker\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\nplt.title(\"Top 10 Most Frequent Speakers\")\nplt.xlabel(\"Speaker\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Party Affiliation vs. Truthfulness\nparty_truth_table = pd.crosstab(train_df[\"Party\"], train_df[\"Simplified_Label\"], normalize=\"index\")\nparty_truth_table.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=[truthfulness_colors[label] for label in party_truth_table.columns])\nplt.title(\"Truthfulness Distribution by Political Party\")\nplt.xlabel(\"Political Party\")\nplt.ylabel(\"Proportion of Statements\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Subject Analysis\ntop_subjects = train_df[\"Subject\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Subjects in the Dataset\")\nplt.xlabel(\"Subject\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Word Clouds\ntrue_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"true\"][\"Subject\"].dropna())\nfalse_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"false\"][\"Subject\"].dropna())\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nwordcloud_true = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Blues\").generate(true_subjects)\naxes[0].imshow(wordcloud_true, interpolation=\"bilinear\")\naxes[0].set_title(\"Subjects in True Statements\", color=\"blue\")\naxes[0].axis(\"off\")\n\nwordcloud_false = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Reds\").generate(false_subjects)\naxes[1].imshow(wordcloud_false, interpolation=\"bilinear\")\naxes[1].set_title(\"Subjects in False Statements\", color=\"red\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nHandling Missing Values\nWe remove or impute missing values in the dataset to ensure models are trained on clean data.\n\n# State-Level Trends (False Statements) - Removing \"Unknown\"\n\n# Count false statements per state and remove \"Unknown\"\nfalse_statements_by_state = train_df[train_df[\"Simplified_Label\"] == \"false\"][\"State\"].value_counts()\nfalse_statements_by_state = false_statements_by_state[false_statements_by_state.index != \"Unknown\"].head(15)\n\n# Plot updated bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(x=false_statements_by_state.index, y=false_statements_by_state.values, color=\"red\")\nplt.title(\"Top 15 States with Most False Statements (Excluding Unknown)\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of False Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Job Title Trends - Removing \"Unknown\"\n\n# Remove \"Unknown\" job titles\ntop_jobs = train_df[train_df[\"Speaker_Job\"] != \"Unknown\"][\"Speaker_Job\"].value_counts().head(10)\n\n# Plot updated bar chart for top job titles\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Speaker Job Titles (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Truthfulness breakdown by job title\njob_truth_table = pd.crosstab(train_df[\"Speaker_Job\"], train_df[\"Simplified_Label\"], normalize=\"index\")\njob_truth_table = job_truth_table.loc[top_jobs.index]  # Filter only top 10 jobs\n\n# Stacked bar chart with color coding\njob_truth_table.plot(kind=\"bar\", stacked=True, figsize=(12, 6), color=[truthfulness_colors[label] for label in job_truth_table.columns])\nplt.title(\"Truthfulness Breakdown by Job Title (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Proportion of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.show()"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#logistic-regression-models",
    "href": "projects/project1/Liar_dataset_final_full.html#logistic-regression-models",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nLogistic Regression: A linear model that estimates the probability of class membership using a logistic function and optimized coefficients, with hyperparameters tuned to balance model complexity and performance.\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df[\"Simplified_Label\"])\ny_test = label_encoder.transform(test_df[\"Simplified_Label\"])\n\n# Feature Extraction: TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), sublinear_tf=True)\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nparam_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\nlog_reg.fit(X_train_tfidf, y_train)\ny_pred = log_reg.predict(X_test_tfidf)\nprint(\"Baseline Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBaseline Logistic Regression Accuracy: 0.43567482241515393\n              precision    recall  f1-score   support\n\n           0       0.39      0.30      0.34       341\n           1       0.44      0.45      0.44       477\n           2       0.46      0.52      0.49       449\n\n    accuracy                           0.44      1267\n   macro avg       0.43      0.42      0.42      1267\nweighted avg       0.43      0.44      0.43      1267\n\n\n\n\n# Feature Extraction: Categorical Encoding\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\nX_train_categorical = one_hot_encoder.fit_transform(train_df[categorical_features])\nX_test_categorical = one_hot_encoder.transform(test_df[categorical_features])\n\n# Feature Extraction: Numerical Scaling\nscaler = StandardScaler()\nX_train_numeric = scaler.fit_transform(train_df[numeric_features])\nX_test_numeric = scaler.transform(test_df[numeric_features])\n\n# Combine Features\nX_train_combined = hstack([X_train_tfidf, X_train_categorical, X_train_numeric])\nX_test_combined = hstack([X_test_tfidf, X_test_categorical, X_test_numeric])\n\n\n\n# Logistic Regression Model Training\nlog_param_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), log_param_grid, cv=5)\nlog_reg.fit(X_train_combined, y_train)\n\n# Predictions\ny_pred = log_reg.predict(X_test_combined)\n\n# Evaluation\nprint(\"Best Logistic Regression Parameters:\", log_reg.best_params_)\nprint(\"Logistic Regression Accuracy with Added Metadata:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBest Logistic Regression Parameters: {'C': 0.1}\nLogistic Regression Accuracy with Added Metadata: 0.468034727703236\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.48      0.60      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267\n\n\n\n\n# Logistic Regression with TF-IDF (n-grams)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), sublinear_tf=True)\nX_train_tfidf_ngram = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf_ngram = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nX_train_combined_ngram = hstack([X_train_tfidf_ngram, X_train_categorical, X_train_numeric])\nX_test_combined_ngram = hstack([X_test_tfidf_ngram, X_test_categorical, X_test_numeric])\n\nlog_reg.fit(X_train_combined_ngram, y_train)\ny_pred_log_ngram = log_reg.predict(X_test_combined_ngram)\nprint(\"Logistic Regression with TF-IDF (n-grams) Accuracy:\", accuracy_score(y_test, y_pred_log_ngram))\nprint(classification_report(y_test, y_pred_log_ngram))\n\nLogistic Regression with TF-IDF (n-grams) Accuracy: 0.46724546172059983\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.47      0.59      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#support-vector-machine",
    "href": "projects/project1/Liar_dataset_final_full.html#support-vector-machine",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\nSupport Vector Machine: A classifier that identifies the optimal hyperplane separating classes by maximizing the margin between data points, with hyperparameters tuned to control the trade-off between misclassification and margin width.\n\n# Grid Search for SVM\nsvm_param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\nsvm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\nsvm_grid_search.fit(X_train_combined, y_train)\nbest_svm_params = svm_grid_search.best_params_\nprint(\"Best SVM Parameters:\", best_svm_params)\n\nBest SVM Parameters: {'C': 0.1, 'kernel': 'linear'}\n\n\n\n# Train SVM Model\nsvm_model = SVC(kernel=\"linear\", C=1.0)\nsvm_model.fit(X_train_combined, y_train)\ny_pred_svm = svm_model.predict(X_test_combined)\nprint(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm))\n\nSVM Accuracy: 0.43962115232833465\n              precision    recall  f1-score   support\n\n           0       0.41      0.39      0.40       341\n           1       0.45      0.41      0.43       477\n           2       0.45      0.51      0.48       449\n\n    accuracy                           0.44      1267\n   macro avg       0.44      0.44      0.44      1267\nweighted avg       0.44      0.44      0.44      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#random-forest",
    "href": "projects/project1/Liar_dataset_final_full.html#random-forest",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Random Forest",
    "text": "Random Forest\nRandom Forest: An ensemble learning method that constructs multiple decision trees on bootstrapped data and aggregates their predictions for improved accuracy and robustness, with hyperparameters tuned to manage tree depth and the number of features considered.\n\n\n# Define hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [200, 500],\n    \"max_depth\": [10, 15, None],\n    \"min_samples_split\": [2, 5],\n    \"min_samples_leaf\": [1, 2]\n}\n\n# Perform Grid Search with 5-fold cross-validation\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, n_jobs=-1, verbose=1)\nrf_grid_search.fit(X_train_combined, y_train)\n\n# Get the best parameters and the best model\nbest_rf_params = rf_grid_search.best_params_\nbest_rf_model = rf_grid_search.best_estimator_\n\n# Evaluate the best Random Forest model\ny_pred_rf_best = best_rf_model.predict(X_test_combined)\nrf_best_accuracy = accuracy_score(y_test, y_pred_rf_best)\nrf_best_report = classification_report(y_test, y_pred_rf_best)\n\nprint(\"Random Forest Accuracy:\", rf_best_accuracy)\nprint(\"Classification Report:\\n\", rf_best_report)\n\n\nFitting 5 folds for each of 24 candidates, totalling 120 fits\nRandom Forest Accuracy: 0.5706393054459353\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.64      0.45      0.52       341\n           1       0.53      0.60      0.56       477\n           2       0.58      0.64      0.61       449\n\n    accuracy                           0.57      1267\n   macro avg       0.58      0.56      0.56      1267\nweighted avg       0.58      0.57      0.57      1267\n\n\n\n\nAnalysis of Experimental Results\nThe experiments demonstrate the effects of different feature engineering methods and models:\n\nLogistic Regression + TF-IDF: Baseline performance achieved ~43.6% accuracy.\nFeature Engineering + TF-IDF: Adding metadata features (speaker, job title, party, history counts) improved accuracy to ~46.8%.\nTrigram TF-IDF: Adding trigrams gave a modest boost, achieving ~47.2% accuracy.\nSupport Vector Machine: Performed comparably (~46.6%), showing linear models have limitations on this dataset.\nRandom Forest + Trigram TF-IDF + Metadata: Achieved the best performance with 57% accuracy and 0.56 macro F1-score. The model handled complex interactions between textual and metadata features effectively.\n\nKey Insight: Incorporating metadata significantly improves classification accuracy. Ensemble models like Random Forest are especially well-suited to capturing these patterns.\nFurther analysis showed that false statements were harder to classify compared to true ones. Future improvements might include advanced language models or hybrid techniques that better capture nuanced language.\n\n\nConclusion\nThis project demonstrates that a combination of NLP techniques and careful feature engineering can effectively address the challenges posed by misinformation in political discourse. The LIAR dataset serves as a valuable resource for developing automated fact-checking systems. While complex models like random forests offer superior performance, traditional models remain competitive and efficient, especially with limited computational resources. The demonstrated effectiveness of incorporating metadata and leveraging models like Random Forest highlights the potential for these classifiers to enhance preliminary screening within real-world automated fact-checking systems. Our findings provide a foundation for future work in enhancing the reliability and scalability of automated fact-checking systems."
  },
  {
    "objectID": "projects/project2/hw1_questions.html",
    "href": "projects/project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe matching grant letters varied along multiple dimensions, including the size of the match ($1:$1, $2:$1, or $3:$1), the maximum amount of the matching gift, and the suggested donation amount. These variations allowed the researchers to isolate the effects of perceived price changes and test whether more generous matches led to higher response rates or larger donation amounts. By embedding this experiment in a real fundraising campaign for a politically active nonprofit, the authors were able to gather behavioral data with high external validity.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#introduction",
    "href": "projects/project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe matching grant letters varied along multiple dimensions, including the size of the match ($1:$1, $2:$1, or $3:$1), the maximum amount of the matching gift, and the suggested donation amount. These variations allowed the researchers to isolate the effects of perceived price changes and test whether more generous matches led to higher response rates or larger donation amounts. By embedding this experiment in a real fundraising campaign for a politically active nonprofit, the authors were able to gather behavioral data with high external validity.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#data",
    "href": "projects/project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\") # &lt;-- just to make outputs look clean!\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n# Variables to test on\nbalance_vars = ['mrm2', 'hpa', 'freq']\n\nfor var in balance_vars:\n    # Drop missing values\n    group_A = df[df['treatment'] == 1][var].dropna()\n    group_B = df[df['treatment'] == 0][var].dropna()\n    \n    # Compute means, stds, Ns for formula\n    X_A, X_B = group_A.mean(), group_B.mean()\n    S_A, S_B = group_A.std(ddof=1), group_B.std(ddof=1)\n    N_A, N_B = len(group_A), len(group_B)\n    \n    # t-test using formula\n    t_stat = (X_A - X_B) / np.sqrt((S_A**2 / N_A) + (S_B**2 / N_B))\n\n    # Run OLS regression\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    pval = model.pvalues['treatment']\n\n    # Output\n    print(\"\\n\" + \"=\"*65)\n    print(f\"Balance test for: `{var}`\")\n    print(f\"\\nMean (Treatment): {X_A:.3f}, Mean (Control): {X_B:.3f}\")\n    print(f\"t = {t_stat:.3f}\")\n    \n    if abs(t_stat) &gt; 1.96:\n        print(\"The absolute t-statistic exceeds 1.96, indicating a statistically significant difference between treatment and control groups at the 5% level.\")\n    else:\n        print(\"The absolute t-statistic is less than 1.96, suggesting no statistically significant difference between groups — balance is likely achieved.\")\n\n    print(f\"\\nRegression coefficient (treatment): {coef:.4f}, p-value: {pval:.3f}\")\n    if pval &lt; 0.05:\n        print(f\"The p-value is less than 0.05 — this suggests imbalance for `{var}`.\")\n    else:\n        print(f\"The p-value is greater than 0.05 — no evidence of imbalance for `{var}`.\")\n\n\n=================================================================\nBalance test for: `mrm2`\n\nMean (Treatment): 13.012, Mean (Control): 12.998\nt = 0.120\nThe absolute t-statistic is less than 1.96, suggesting no statistically significant difference between groups — balance is likely achieved.\n\nRegression coefficient (treatment): 0.0137, p-value: 0.905\nThe p-value is greater than 0.05 — no evidence of imbalance for `mrm2`.\n\n=================================================================\nBalance test for: `hpa`\n\nMean (Treatment): 59.597, Mean (Control): 58.960\nt = 0.970\nThe absolute t-statistic is less than 1.96, suggesting no statistically significant difference between groups — balance is likely achieved.\n\nRegression coefficient (treatment): 0.6371, p-value: 0.345\nThe p-value is greater than 0.05 — no evidence of imbalance for `hpa`.\n\n=================================================================\nBalance test for: `freq`\n\nMean (Treatment): 8.035, Mean (Control): 8.047\nt = -0.111\nThe absolute t-statistic is less than 1.96, suggesting no statistically significant difference between groups — balance is likely achieved.\n\nRegression coefficient (treatment): -0.0120, p-value: 0.912\nThe p-value is greater than 0.05 — no evidence of imbalance for `freq`.\n\n\n\n\n\n\n\n\nOLS Tables\n\n\n\n\n\n\nbalance_vars = ['mrm2', 'hpa', 'freq']\n\nfor var in balance_vars:\n    print(f\"### Full OLS Summary for `{var}`\")\n    print(model.summary())\n    print(\":::\\n\")\n\n### Full OLS Summary for `mrm2`\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.912\nTime:                        20:55:08   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n:::\n\n### Full OLS Summary for `hpa`\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.912\nTime:                        20:55:08   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n:::\n\n### Full OLS Summary for `freq`\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   freq   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01230\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.912\nTime:                        20:55:08   Log-Likelihood:            -1.9292e+05\nNo. Observations:               50083   AIC:                         3.858e+05\nDf Residuals:                   50081   BIC:                         3.859e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      8.0473      0.088     91.231      0.000       7.874       8.220\ntreatment     -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\nOmnibus:                    49107.114   Durbin-Watson:                   2.016\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3644795.393\nSkew:                           4.707   Prob(JB):                         0.00\nKurtosis:                      43.718   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n:::"
  },
  {
    "objectID": "projects/project2/hw1_questions.html#experimental-results",
    "href": "projects/project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n# Prepare the data\ndonation_rate = df.groupby('treatment')['gave'].mean().reset_index()\ndonation_rate['group'] = donation_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\n\n# Plot\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(6, 5))\n\nax = sns.barplot(x='group', y='gave', data=donation_rate, palette='Blues_d')\n\n# Add labels on bars\nfor index, row in donation_rate.iterrows():\n    ax.text(index, row['gave'] + 0.00001, f\"{row['gave']:.2%}\", ha='center', va='bottom', fontsize=12)\n\n# Axis and title\nax.set_ylabel(\"Proportion Donated\", fontsize=12)\nax.set_xlabel(\"\")\nax.set_title(\"Donation Rates by Group\", fontsize=14)\nax.set_ylim(0, 0.05)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Subset data for Regression and t-test\nsubset = df[(df['control'] == 1) | (df['treatment'] == 1)].copy()\nsubset['group'] = df['treatment']\n\n# t-test\ncontrol_group = subset[subset['group'] == 0]['gave']\ntreat_group = subset[subset['group'] == 1]['gave']\n\n# Use stats library instead of manual formula\nt_stat, p_val = stats.ttest_ind(treat_group, control_group, equal_var=False)\n\nprint(\"\\n=== T-Test: Treatment vs Control ===\")\nprint(f\"Control response rate: {control_group.mean():.3f}\")\nprint(f\"Treatment response rate: {treat_group.mean():.3f}\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_val:.4f}\")\n\n# Regression\nmodel = smf.ols(\"gave ~ group\", data=subset).fit()\n\n# Print results\nprint(\"\\n=== OLS Regression Summary ===\")\nprint(model.summary())\n\n\n=== T-Test: Treatment vs Control ===\nControl response rate: 0.018\nTreatment response rate: 0.022\nt-statistic: 3.209\np-value: 0.0013\n\n=== OLS Regression Summary ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        20:55:08   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ngroup          0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIn this regression, I found that people in the treatment group were significantly more likely to donate compared to those in the control group. The increase in donation likelihood is small, but the result is statistically significant, meaning it is unlikely to be due to random chance. This is shown by the p-value of 0.002 for the treatment variable, which is well below the common 0.05 threshold for significance. The coefficient of 0.0042 means that the treatment increased the probability of donating by about 0.42 percentage points. Even though most people did not donate overall, being told their gift would be matched made a small but noticeable difference.\n\n# Probit Model\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit()\nprint(probit_model.summary())\nprint(\"\\n\")\n\n# Marginal effect of Probit Model (to show 0.004!)\nmfx = probit_model.get_margeff(at='mean')\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        20:55:08   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                              mean\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.121      0.002       0.002       0.007\n==============================================================================\n\n\nI also ran a Probit regression to estimate the effect of being in the treatment group on the likelihood of donating. The marginal effect of treatment is about 0.0043, meaning that receiving the matching message increased the probability of donating by 0.43 percentage points. This effect is statistically significant with a p-value of 0.002, suggesting the treatment had a real impact on behavior, even if the absolute increase was small. This result further solidifies the results found from the T-Test and the logistic regression.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. In their paper, Karlan and List say “Yet, while the match treatments relative to a control group increase the probability of donating, larger match ratios—$3:$1 and $2:$1—relative to smaller match ratios ($1:$1) had no additional impact.” This was a surprising conclusion to me, so I tested the differences between match rates below.\n\n# Filter only those with a ratio value\nratio_df = df[df['ratio'].notna()]\n\n# Create groups for each ratio level\ngroup_1_1 = ratio_df[ratio_df['ratio'] == 1]['gave']\ngroup_2_1 = ratio_df[ratio_df['ratio'] == 2]['gave']\ngroup_3_1 = ratio_df[ratio_df['ratio'] == 3]['gave']\n\n# Run t-tests using stats\nt_21_vs_11, p_21_vs_11 = stats.ttest_ind(group_2_1, group_1_1, equal_var=False)\nt_31_vs_11, p_31_vs_11 = stats.ttest_ind(group_3_1, group_1_1, equal_var=False)\nt_31_vs_21, p_31_vs_21 = stats.ttest_ind(group_3_1, group_2_1, equal_var=False)\n\n# Print results\nprint(\"\\n=== T-Tests: Match Ratio Comparisons ===\")\nprint(f\"2:1 vs 1:1 — t = {t_21_vs_11:.3f}, p = {p_21_vs_11:.4f}\")\nprint(f\"3:1 vs 1:1 — t = {t_31_vs_11:.3f}, p = {p_31_vs_11:.4f}\")\nprint(f\"3:1 vs 2:1 — t = {t_31_vs_21:.3f}, p = {p_31_vs_21:.4f}\")\n\n\n=== T-Tests: Match Ratio Comparisons ===\n2:1 vs 1:1 — t = 0.965, p = 0.3345\n3:1 vs 1:1 — t = 1.015, p = 0.3101\n3:1 vs 2:1 — t = 0.050, p = 0.9600\n\n\nThe t-tests showed no statistically significant differences between the groups (all p-values &gt; 0.05), indicating that larger match ratios did not significantly increase the probability of donating. These results support the authors’ claim that, although announcing a match increases giving, increasing the match size (from 1:1 to 2:1 or 3:1) has no further effect. Interesting!\n\n# Make ratio1\ndf['ratio1'] = (df['ratio'] == 1).astype(int)\n\n# Filter to just treatment group with non-null ratio values\nsubset = df[df['ratio'].notna()].copy()\n\n# Run the regression with 1:1 as the baseline\nmodel = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=subset).fit()\nprint(model.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                   gave   R-squared (uncentered):                   0.016\nModel:                            OLS   Adj. R-squared (uncentered):              0.016\nMethod:                 Least Squares   F-statistic:                              266.5\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):                   1.28e-171\nTime:                        20:55:08   Log-Likelihood:                          26499.\nNo. Observations:               50083   AIC:                                 -5.299e+04\nDf Residuals:                   50080   BIC:                                 -5.297e+04\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     15.357      0.000       0.018       0.023\nratio2         0.0226      0.001     16.753      0.000       0.020       0.025\nratio3         0.0227      0.001     16.823      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    59526.384   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4230365.728\nSkew:                           6.688   Prob(JB):                         0.00\nKurtosis:                      45.992   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo further validate this claim, I created a new variable (ratio1), representing a 1:1 match. I then used it along with ratio2 and ratio3 in a regression, taking out the intercept to avoid multicollinearity. The estimated donation rates were 2.07% for the 1:1 match group, 2.26% for the 2:1 group, and 2.27% for the 3:1 group. While the differences are statistically significant due to the large sample size, they are extremely small in magnitude. This supports the authors’ finding that increasing the match ratio beyond 1:1 does not meaningfully increase donation likelihood.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# Compare the amount donated for EVERYONE\ntreat_group = df[df['treatment'] == 1]['amount'].dropna()\ncontrol_group = df[df['control'] == 1]['amount'].dropna()\n\n# Run t-tests using stats\nt_stat, p_val = stats.ttest_ind(treat_group, control_group, equal_var=False)\n\n# Print results\nprint(\"=== T-Test: Amount Donated ~ Treatment ===\")\nprint(f\"Mean (Control): {control_group.mean():.3f}\")\nprint(f\"Mean (Treatment): {treat_group.mean():.3f}\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_val:.4f}\")\n\n=== T-Test: Amount Donated ~ Treatment ===\nMean (Control): 0.813\nMean (Treatment): 0.967\nt-statistic: 1.918\np-value: 0.0551\n\n\nI ran both a t-test to compare the average donation amount between the treatment and control groups. The treatment group gave slightly more on average, and the difference was statistically significant. However, the increase was modest in size. This tells us that while being in the treatment group (with a matching grant) not only increases the likelihood of donating, it also slightly increases how much people give — suggesting that matching grants may influence both whether and how much people donate.\n\n# Now compare amounts donated only considering people who donated\ndonated_df = df[(df['gave'] == 1) & ((df['treatment'] == 1) | (df['control'] == 1))].copy()\ndonated_df['group'] = donated_df['treatment']  # 1 if treatment, 0 if control\n\n# Find donation amounts\ntreat_amt = donated_df[donated_df['group'] == 1]['amount']\ncontrol_amt = donated_df[donated_df['group'] == 0]['amount']\n\n# Run t-test using stats\nt_stat, p_val = stats.ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Print results\nprint(\"=== T-Test: Amount (Conditional on Giving) ===\")\nprint(f\"Mean (Treatment): {treat_amt.mean():.2f}\")\nprint(f\"Mean (Control): {control_amt.mean():.2f}\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_val:.4f}\")\n\n=== T-Test: Amount (Conditional on Giving) ===\nMean (Treatment): 43.87\nMean (Control): 45.54\nt-statistic: -0.585\np-value: 0.5590\n\n\nI examined whether people in the treatment group donated more money than those in the control group, conditional on having donated. The average donation amount was slightly lower in the treatment group ($43.87) than in the control group ($45.54), but this difference was not statistically significant (p = 0.559). This suggests that while the treatment increased the likelihood of giving, it did not affect how much people gave once they decided to donate. Since this analysis is limited to those who self-selected into giving, the treatment coefficient does not have a clear causal interpretation — the act of giving may already differ across groups due to unobserved selection.\n\n# Filter for people who donated\ndonors = df[(df['gave'] == 1) & ((df['control'] == 1) | (df['treatment'] == 1))].copy()\ndonors['group'] = donors['treatment'].map({0: 'Control', 1: 'Treatment'})\n\n# Create plot\nsns.set(style=\"whitegrid\")\n\n# Create subplots\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Plot histogram for each group\nfor i, grp in enumerate(['Control', 'Treatment']):\n    data = donors[donors['group'] == grp]['amount']\n    mean_val = data.mean()\n    \n    sns.histplot(data, bins=30, ax=axes[i], kde=False, color='skyblue')\n    axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Mean = ${mean_val:.2f}')\n    axes[i].set_title(f\"{grp} Group\")\n    axes[i].set_xlabel(\"Donation Amount\")\n    axes[i].set_ylabel(\"Number of Donors\")\n    axes[i].legend()\n\nplt.suptitle(\"Donation Amounts (Among Donors Only)\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project2/hw1_questions.html#simulation-experiment",
    "href": "projects/project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n# Set true values\np_control = 0.018\np_treatment = 0.022\n\n# Simulate data\nnp.random.seed(1234)\ncontrol = np.random.binomial(1, p_control, size=100_000)\ntreatment = np.random.binomial(1, p_treatment, size=10_000)\n\n# Compute the difference in each simulation\ndifferences = treatment - control[:10_000]\n\n# Compute the cumulative average of differences\ncum_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cum_avg, label='Cumulative Average Difference')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.xlabel('Number of Observations')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Law of Large Numbers: Cumulative Avg of Treatment − Control')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe plot shows the cumulative average difference in donation outcomes between a simulated treatment group (with p = 0.022) and control group (p = 0.018), across 10,000 samples. Even though the average is very volatile at first, as the number of observations increases, the cumulative average stabilizes and converges toward the true difference of 0.004. This is the Law of Large Numbers! With enough observations, the sample average converges to the expected value and shows the true effect of the treatment.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\n# Set true values\np_control = 0.018\np_treatment = 0.022\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nnp.random.seed(1234)\n\n# Set up the plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Run simulation for each sample size\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, p_control, size=n)\n        treatment = np.random.binomial(1, p_treatment, size=n)\n        diffs.append(treatment.mean() - control.mean())\n    \n    # Plot histogram of average differences\n    sns.histplot(diffs, bins=30, kde=False, ax=axes[i], color='skyblue')\n    axes[i].axvline(0.004, color='red', linestyle='--', label='True Diff = 0.004')\n    axes[i].axvline(0, color='black', linestyle=':', label='Zero')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Difference in Means\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.suptitle(\"Sampling Distribution of Differences at Varying Sample Sizes\", fontsize=14, y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\nThese histograms show the sampling distributions of the difference in donation rates between treatment and control groups at different sample sizes (50, 200, 500, 1000). As the sample size increases, the distribution becomes narrower (less variation) and increasingly centers around the true treatment effect of 0.004. At small sample sizes, the distribution is wide and zero is often near the center, meaning we might not detect a treatment effect. But at larger sample sizes, the distribution shifts away from zero, and it becomes clear that the treatment increases donation probability. This highlights how statistical significance depends not just on effect size, but also on sample size."
  },
  {
    "objectID": "projects/project2/hw1_questions.html#conclusion",
    "href": "projects/project2/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis explored how subtle changes in charitable fundraising messages can influence donor behavior. Across a range of statistical tests and simulations, the results consistently showed that while offering a matching donation increases the likelihood that someone donates, increasing the match ratio beyond a basic 1:1 offer provides little additional benefit. The treatment’s effect was primarily observed in encouraging people to give, rather than changing how much they donated. These findings highlight the importance of behavioral nudges in shaping economic decisions, the role of sample size in detecting effects, and the value of empirical evidence in challenging fundraising assumptions. Thank you for stopping by!"
  }
]