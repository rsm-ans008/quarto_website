[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anoop Singh",
    "section": "",
    "text": "Education\n\nMS Business Analytics | Rady School of Management | 06/2025\n\nBS Business Economics | UCSD | 06/2024\n\n\n\n\nWork History\n\nBusiness Analyst | Thermo Fisher Scientific | 03/2025 – Present\n\nTeaching Assistant | Customer Analytics | 09/2024 – Present\n\nProject Manager | Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\nFinance Intern | Bonduelle Fresh Americas | 06/2023 – 08/2023\n\nBusiness Development Intern | OmniSync Inc | 06/2022 – 08/2022\n\n\n\n\nLeadership\n\nVice President | Triton Consulting Group | 2023 – 2024\n\nProject Manager | Triton Consulting Group | 2022 – 2023\n\nMember | Rady Data Analytics Club | 2024 – 2025\n\n\n\n\nInterests & Tools\n\nLanguages & Tools: Python, SQL, R, Tableau, Power BI, PySpark, Hadoop, Databricks, AWS, Snowflake\n\nFun Stuff: Pickleball, Snowboarding, Hiking, Baking, Board Games, Machine Learning, Big Data"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Anoop Singh",
    "section": "",
    "text": "MS Business Analytics | Rady School of Management | 06/2025\n\nBS Business Economics | UCSD | 06/2024"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Anoop Singh",
    "section": "",
    "text": "Business Analyst | Thermo Fisher Scientific | 03/2025 – Present\n\nTeaching Assistant | Customer Analytics | 09/2024 – Present\n\nProject Manager | Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\nFinance Intern | Bonduelle Fresh Americas | 06/2023 – 08/2023\n\nBusiness Development Intern | OmniSync Inc | 06/2022 – 08/2022"
  },
  {
    "objectID": "index.html#leadership",
    "href": "index.html#leadership",
    "title": "Anoop Singh",
    "section": "",
    "text": "Vice President | Triton Consulting Group | 2023 – 2024\n\nProject Manager | Triton Consulting Group | 2022 – 2023\n\nMember | Rady Data Analytics Club | 2024 – 2025"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/index.html#sub-header",
    "href": "projects/project2/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download Resume \n\n\nAnoop Singh\n\n\nEducation\n\n\n\nMaster of Science in Business Analytics, Rady School of Management, University of California, San Diego, CA | 06/2025 | GPA: 3.9/4.0\n\n\n\nRelevant Courses: Unstructured Data, SQL and ETL, Collecting and Analyzing Large Data, Customer Analytics\n\n\n\nBachelor of Science in Business Economics, University of California, San Diego, CA | 06/2024 | GPA: 3.5/4.0\n\n\n\n\nExperience\n\n\n\nSupply Chain Data Analyst, Thermo Fisher Scientific, San Diego, CA | 03/2025 – Present\n\n\n\nDeveloped a machine learning model in Databricks to predict supply chain risk, improving procurement efficiency and enabling proactive decision-making.\n\n\nBuilt an end-to-end data pipeline integrating 5 years of supply chain data into a centralized data lake, enhancing analytics for waste reduction.\n\n\n\nProject Manager, Gawad Kalinga, Philippines | 03/2024 – 06/2024\n\n\n\nImplemented a solar power generator system with digital monitoring, reducing electricity costs by 80% in a rural community.\n\n\nLed market research and feasibility analysis for producing/distributing solar lanterns to promote economic self-sufficiency.\n\n\n\nFinance Intern, Bonduelle Fresh Americas, Los Angeles, CA | 06/2023 – 08/2023\n\n\n\nCreated a Tableau performance analysis tool to cut promotional inefficiencies, saving $15,000 annually.\n\n\nBuilt a regression model for cost prediction, enabling better contract management and supply chain decisions.\n\n\n\nBusiness Development Intern, OmniSync Inc, San Diego, CA | 06/2022 – 08/2022\n\n\n\nSupported 100+ startups in securing SBIR/STTR grants through compliance and proposal support.\n\n\nAutomated engagement workflows in Python, improving client retention and outreach using database insights.\n\n\n\n\n\nProjects\n\n\n\nFood Insecurity Dashboard, Business Intelligence Systems, UCSD | 09/2024 – 12/2024\n\n\n\nBuilt an interactive Tableau dashboard visualizing food insecurity in 84,000+ U.S. census tracts.\n\n\nCleaned over 1M records using SQL and Tableau Prep, delivering insights to nonprofits and advocacy groups.\n\n\n\nAnime Recommender System, Web Mining & Recommender Systems, UCSD | 09/2024 – 12/2024\n\n\n\nBuilt a personalized recommender model using 7.8M user-anime interactions; achieved RMSE of 1.56.\n\n\nPerformed EDA on 12,000+ titles to improve input quality and model accuracy.\n\n\n\n\n\nProfessional Affiliations & Leadership\n\n\n\nTeaching Assistant, Customer Analytics, UCSD | 09/2024 – Present\n\n\n\nLed instruction for 300+ students in segmentation techniques and R-based statistical modeling.\n\n\n\nVP of Internal Development, Triton Consulting Group, UCSD | 06/2023 – 06/2024\n\n\n\nDeveloped R-based customer segmentation and marketing strategy that increased boutique revenue by 10%.\n\n\nAutomated data governance workflows in Python, cutting cleanup time from hours to minutes.\n\n\n\n\n\nSkills & Interests\n\n\n\nProgramming & Analytics: Python, SQL, R, PySpark, Spark, ETL, A/B Testing, Tableau, Power BI, Excel\n\n\nCloud & Big Data Tools: AWS (EMR, SageMaker), Azure, Hadoop, Snowflake, Databricks, GitHub, CRM\n\n\nInterests: Pickleball, Snowboarding, Hiking, Weightlifting, Board Games, Baking, Machine Learning, Big Data"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of Cars\n\n\n\n\n\n\nAnoop Singh\n\n\nApr 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Detection and Automated Fact-Checking on the LIAR Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#interests-tools",
    "href": "index.html#interests-tools",
    "title": "Anoop Singh",
    "section": "",
    "text": "Languages & Tools: Python, SQL, R, Tableau, Power BI, PySpark, Hadoop, Databricks, AWS, Snowflake\n\nFun Stuff: Pickleball, Snowboarding, Hiking, Baking, Board Games, Machine Learning, Big Data"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html",
    "href": "projects/project1/Liar_dataset_final.html",
    "title": "Fake News Classification with NLP",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport re\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.impute import SimpleImputer"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#load-and-preprocess-data",
    "href": "projects/project1/Liar_dataset_final.html#load-and-preprocess-data",
    "title": "Fake News Classification with NLP",
    "section": "Load and Preprocess Data",
    "text": "Load and Preprocess Data\n\n\n# Load dataset\ntrain_df = pd.read_csv(\"liar_dataset/train.tsv\", delimiter=\"\\t\", header=None)\ntest_df = pd.read_csv(\"liar_dataset/test.tsv\", delimiter=\"\\t\", header=None)\n\n# Define column names\ncolumn_names = [\n    \"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\",\n    \"State\", \"Party\", \"Barely_True_Count\", \"False_Count\", \"Half_True_Count\",\n    \"Mostly_True_Count\", \"Pants_On_Fire_Count\", \"Context\"\n]\ntrain_df.columns = column_names\ntest_df.columns = column_names\n\n# Handle NaNs\nnumeric_features = [\"Barely_True_Count\", \"False_Count\", \"Half_True_Count\", \"Mostly_True_Count\", \"Pants_On_Fire_Count\"]\ntrain_df = train_df.dropna(subset=numeric_features)\ntest_df = test_df.dropna(subset=numeric_features)\n\ncategorical_features = [\"Speaker\", \"Speaker_Job\", \"State\", \"Party\", \"Subject\", \"Context\"]\ntrain_df[categorical_features] = train_df[categorical_features].fillna(\"Unknown\")\ntest_df[categorical_features] = test_df[categorical_features].fillna(\"Unknown\")"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#feature-engineering",
    "href": "projects/project1/Liar_dataset_final.html#feature-engineering",
    "title": "Fake News Classification with NLP",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Preprocessing function for statements\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n    return text\n\ntrain_df[\"Processed_Statement\"] = train_df[\"Statement\"].apply(preprocess_text)\ntest_df[\"Processed_Statement\"] = test_df[\"Statement\"].apply(preprocess_text)\n\n# Simplify labels\nlabel_mapping = {\n    \"true\": \"true\", \"mostly-true\": \"true\",\n    \"half-true\": \"somewhat-true\", \"barely-true\": \"somewhat-true\",\n    \"false\": \"false\", \"pants-fire\": \"false\"\n}\ntrain_df[\"Simplified_Label\"] = train_df[\"Label\"].map(label_mapping)\ntest_df[\"Simplified_Label\"] = test_df[\"Label\"].map(label_mapping)"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#exploratory-data-analysis-eda",
    "href": "projects/project1/Liar_dataset_final.html#exploratory-data-analysis-eda",
    "title": "Fake News Classification with NLP",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n# Plot label distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Label\", order=train_df[\"Label\"].value_counts().index)\nplt.title(\"Distribution of Labels in LIAR Dataset\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Statement length column\ntrain_df[\"Statement_Length\"] = train_df[\"Statement\"].apply(lambda x: len(str(x).split()))\n\n# Color mapping for truthfulness\ntruthfulness_colors = {\"false\": \"red\", \"somewhat-true\": \"orange\", \"true\": \"blue\"}\n\n# Label Distribution Analysis\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\nplt.title(\"Distribution of Simplified Truthfulness Labels\")\nplt.xlabel(\"Truthfulness Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.show()\n\n/tmp/ipykernel_15365/4162792726.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\n\n\n\n\n\n\n\n\n\n\n# Speaker Analysis: Most Frequent Speakers\ntop_speakers = train_df[\"Speaker\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\nplt.title(\"Top 10 Most Frequent Speakers\")\nplt.xlabel(\"Speaker\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n/tmp/ipykernel_15365/1034616230.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n# Party Affiliation vs. Truthfulness\nparty_truth_table = pd.crosstab(train_df[\"Party\"], train_df[\"Simplified_Label\"], normalize=\"index\")\nparty_truth_table.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=[truthfulness_colors[label] for label in party_truth_table.columns])\nplt.title(\"Truthfulness Distribution by Political Party\")\nplt.xlabel(\"Political Party\")\nplt.ylabel(\"Proportion of Statements\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Subject Analysis\ntop_subjects = train_df[\"Subject\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Subjects in the Dataset\")\nplt.xlabel(\"Subject\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n/tmp/ipykernel_15365/3302470076.py:4: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n# Word Clouds\ntrue_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"true\"][\"Subject\"].dropna())\nfalse_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"false\"][\"Subject\"].dropna())\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nwordcloud_true = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Blues\").generate(true_subjects)\naxes[0].imshow(wordcloud_true, interpolation=\"bilinear\")\naxes[0].set_title(\"Subjects in True Statements\", color=\"blue\")\naxes[0].axis(\"off\")\n\nwordcloud_false = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Reds\").generate(false_subjects)\naxes[1].imshow(wordcloud_false, interpolation=\"bilinear\")\naxes[1].set_title(\"Subjects in False Statements\", color=\"red\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# State-Level Trends (False Statements) - Removing \"Unknown\"\n\n# Count false statements per state and remove \"Unknown\"\nfalse_statements_by_state = train_df[train_df[\"Simplified_Label\"] == \"false\"][\"State\"].value_counts()\nfalse_statements_by_state = false_statements_by_state[false_statements_by_state.index != \"Unknown\"].head(15)\n\n# Plot updated bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(x=false_statements_by_state.index, y=false_statements_by_state.values, color=\"red\")\nplt.title(\"Top 15 States with Most False Statements (Excluding Unknown)\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of False Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Job Title Trends - Removing \"Unknown\"\n\n# Remove \"Unknown\" job titles\ntop_jobs = train_df[train_df[\"Speaker_Job\"] != \"Unknown\"][\"Speaker_Job\"].value_counts().head(10)\n\n# Plot updated bar chart for top job titles\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Speaker Job Titles (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n/tmp/ipykernel_15365/2681968054.py:8: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n# Truthfulness breakdown by job title\njob_truth_table = pd.crosstab(train_df[\"Speaker_Job\"], train_df[\"Simplified_Label\"], normalize=\"index\")\njob_truth_table = job_truth_table.loc[top_jobs.index]  # Filter only top 10 jobs\n\n# Stacked bar chart with color coding\njob_truth_table.plot(kind=\"bar\", stacked=True, figsize=(12, 6), color=[truthfulness_colors[label] for label in job_truth_table.columns])\nplt.title(\"Truthfulness Breakdown by Job Title (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Proportion of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.show()"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#logistic-regression-models",
    "href": "projects/project1/Liar_dataset_final.html#logistic-regression-models",
    "title": "Fake News Classification with NLP",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df[\"Simplified_Label\"])\ny_test = label_encoder.transform(test_df[\"Simplified_Label\"])\n\n# Feature Extraction: TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), sublinear_tf=True)\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nparam_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\nlog_reg.fit(X_train_tfidf, y_train)\ny_pred = log_reg.predict(X_test_tfidf)\nprint(\"Baseline Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBaseline Logistic Regression Accuracy: 0.43567482241515393\n              precision    recall  f1-score   support\n\n           0       0.39      0.30      0.34       341\n           1       0.44      0.45      0.44       477\n           2       0.46      0.52      0.49       449\n\n    accuracy                           0.44      1267\n   macro avg       0.43      0.42      0.42      1267\nweighted avg       0.43      0.44      0.43      1267\n\n\n\n\n# Feature Extraction: Categorical Encoding\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\nX_train_categorical = one_hot_encoder.fit_transform(train_df[categorical_features])\nX_test_categorical = one_hot_encoder.transform(test_df[categorical_features])\n\n# Feature Extraction: Numerical Scaling\nscaler = StandardScaler()\nX_train_numeric = scaler.fit_transform(train_df[numeric_features])\nX_test_numeric = scaler.transform(test_df[numeric_features])\n\n# Combine Features\nX_train_combined = hstack([X_train_tfidf, X_train_categorical, X_train_numeric])\nX_test_combined = hstack([X_test_tfidf, X_test_categorical, X_test_numeric])\n\n\n\n# Logistic Regression Model Training\nlog_param_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), log_param_grid, cv=5)\nlog_reg.fit(X_train_combined, y_train)\n\n# Predictions\ny_pred = log_reg.predict(X_test_combined)\n\n# Evaluation\nprint(\"Best Logistic Regression Parameters:\", log_reg.best_params_)\nprint(\"Logistic Regression Accuracy with Added Metadata:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBest Logistic Regression Parameters: {'C': 0.1}\nLogistic Regression Accuracy with Added Metadata: 0.468034727703236\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.48      0.60      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267\n\n\n\n\n# Logistic Regression with TF-IDF (n-grams)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), sublinear_tf=True)\nX_train_tfidf_ngram = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf_ngram = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nX_train_combined_ngram = hstack([X_train_tfidf_ngram, X_train_categorical, X_train_numeric])\nX_test_combined_ngram = hstack([X_test_tfidf_ngram, X_test_categorical, X_test_numeric])\n\nlog_reg.fit(X_train_combined_ngram, y_train)\ny_pred_log_ngram = log_reg.predict(X_test_combined_ngram)\nprint(\"Logistic Regression with TF-IDF (n-grams) Accuracy:\", accuracy_score(y_test, y_pred_log_ngram))\nprint(classification_report(y_test, y_pred_log_ngram))\n\nLogistic Regression with TF-IDF (n-grams) Accuracy: 0.46724546172059983\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.47      0.59      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#support-vector-machine",
    "href": "projects/project1/Liar_dataset_final.html#support-vector-machine",
    "title": "Fake News Classification with NLP",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\n\n# Grid Search for SVM\nsvm_param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\nsvm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\nsvm_grid_search.fit(X_train_combined, y_train)\nbest_svm_params = svm_grid_search.best_params_\nprint(\"Best SVM Parameters:\", best_svm_params)\n\nBest SVM Parameters: {'C': 0.1, 'kernel': 'linear'}\n\n\n\n# Train SVM Model\nsvm_model = SVC(kernel=\"linear\", C=1.0)\nsvm_model.fit(X_train_combined, y_train)\ny_pred_svm = svm_model.predict(X_test_combined)\nprint(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm))\n\nSVM Accuracy: 0.43962115232833465\n              precision    recall  f1-score   support\n\n           0       0.41      0.39      0.40       341\n           1       0.45      0.41      0.43       477\n           2       0.45      0.51      0.48       449\n\n    accuracy                           0.44      1267\n   macro avg       0.44      0.44      0.44      1267\nweighted avg       0.44      0.44      0.44      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final.html#random-forest",
    "href": "projects/project1/Liar_dataset_final.html#random-forest",
    "title": "Fake News Classification with NLP",
    "section": "Random Forest",
    "text": "Random Forest\n\n\n# Define hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [200, 500],\n    \"max_depth\": [10, 15, None],\n    \"min_samples_split\": [2, 5],\n    \"min_samples_leaf\": [1, 2]\n}\n\n# Perform Grid Search with 5-fold cross-validation\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, n_jobs=-1, verbose=1)\nrf_grid_search.fit(X_train_combined, y_train)\n\n# Get the best parameters and the best model\nbest_rf_params = rf_grid_search.best_params_\nbest_rf_model = rf_grid_search.best_estimator_\n\n# Evaluate the best Random Forest model\ny_pred_rf_best = best_rf_model.predict(X_test_combined)\nrf_best_accuracy = accuracy_score(y_test, y_pred_rf_best)\nrf_best_report = classification_report(y_test, y_pred_rf_best)\n\nprint(\"Random Forest Accuracy:\", rf_best_accuracy)\nprint(\"Classification Report:\\n\", rf_best_report)\n\n\nFitting 5 folds for each of 24 candidates, totalling 120 fits\n\n\n/opt/conda/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n\n\nRandom Forest Accuracy: 0.5706393054459353\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.64      0.45      0.52       341\n           1       0.53      0.60      0.56       477\n           2       0.58      0.64      0.61       449\n\n    accuracy                           0.57      1267\n   macro avg       0.58      0.56      0.56      1267\nweighted avg       0.58      0.57      0.57      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html",
    "href": "projects/project1/Liar_dataset_final_full.html",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "",
    "text": "In an era of rapidly spreading misinformation, automated fact‐checking has become a critical tool for evaluating the veracity of public statements. This project leverages the LIAR dataset, a widely recognized benchmark for fake news detection, to build models that classify political statements into veracity categories. We explore a range of natural language processing (NLP) machine learning approaches. We hypothesize that integrating contextual metadata with textual features enhances the accuracy of automated fake news detection, outperforming models relying solely on textual information. Our experiments reveal that baseline classifiers such as Logistic Regression can perform relatively well and are improved when more advanced machine learning methods such as Random Forests are implemented. Overall, our findings underscore the trade-offs between model complexity, training data volume, and computational constraints in the quest for accurate automated fact-checking.\nimport pandas as pd\nimport numpy as np\nimport re\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.impute import SimpleImputer"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#load-and-preprocess-data",
    "href": "projects/project1/Liar_dataset_final_full.html#load-and-preprocess-data",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Load and Preprocess Data",
    "text": "Load and Preprocess Data\n\n\n# Load dataset\ntrain_df = pd.read_csv(\"liar_dataset/train.tsv\", delimiter=\"\\t\", header=None)\ntest_df = pd.read_csv(\"liar_dataset/test.tsv\", delimiter=\"\\t\", header=None)\n\n# Define column names\ncolumn_names = [\n    \"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\",\n    \"State\", \"Party\", \"Barely_True_Count\", \"False_Count\", \"Half_True_Count\",\n    \"Mostly_True_Count\", \"Pants_On_Fire_Count\", \"Context\"\n]\ntrain_df.columns = column_names\ntest_df.columns = column_names\n\n# Handle NaNs\nnumeric_features = [\"Barely_True_Count\", \"False_Count\", \"Half_True_Count\", \"Mostly_True_Count\", \"Pants_On_Fire_Count\"]\ntrain_df = train_df.dropna(subset=numeric_features)\ntest_df = test_df.dropna(subset=numeric_features)\n\ncategorical_features = [\"Speaker\", \"Speaker_Job\", \"State\", \"Party\", \"Subject\", \"Context\"]\ntrain_df[categorical_features] = train_df[categorical_features].fillna(\"Unknown\")\ntest_df[categorical_features] = test_df[categorical_features].fillna(\"Unknown\")"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#feature-engineering",
    "href": "projects/project1/Liar_dataset_final_full.html#feature-engineering",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n# Preprocessing function for statements\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n    return text\n\ntrain_df[\"Processed_Statement\"] = train_df[\"Statement\"].apply(preprocess_text)\ntest_df[\"Processed_Statement\"] = test_df[\"Statement\"].apply(preprocess_text)\n\n# Simplify labels\nlabel_mapping = {\n    \"true\": \"true\", \"mostly-true\": \"true\",\n    \"half-true\": \"somewhat-true\", \"barely-true\": \"somewhat-true\",\n    \"false\": \"false\", \"pants-fire\": \"false\"\n}\ntrain_df[\"Simplified_Label\"] = train_df[\"Label\"].map(label_mapping)\ntest_df[\"Simplified_Label\"] = test_df[\"Label\"].map(label_mapping)\n\n\nText Cleaning\nWe clean the statement text by removing special characters and lowercasing all letters. This normalizes text and improves model generalization."
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#exploratory-data-analysis-eda",
    "href": "projects/project1/Liar_dataset_final_full.html#exploratory-data-analysis-eda",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\n# Plot label distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Label\", order=train_df[\"Label\"].value_counts().index)\nplt.title(\"Distribution of Labels in LIAR Dataset\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Statement length column\ntrain_df[\"Statement_Length\"] = train_df[\"Statement\"].apply(lambda x: len(str(x).split()))\n\n# Color mapping for truthfulness\ntruthfulness_colors = {\"false\": \"red\", \"somewhat-true\": \"orange\", \"true\": \"blue\"}\n\n# Label Distribution Analysis\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train_df, x=\"Simplified_Label\", order=[\"false\", \"somewhat-true\", \"true\"], palette=truthfulness_colors)\nplt.title(\"Distribution of Simplified Truthfulness Labels\")\nplt.xlabel(\"Truthfulness Label\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Analysis: Most Frequent Speakers\ntop_speakers = train_df[\"Speaker\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_speakers.index, y=top_speakers.values, palette=\"viridis\")\nplt.title(\"Top 10 Most Frequent Speakers\")\nplt.xlabel(\"Speaker\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Party Affiliation vs. Truthfulness\nparty_truth_table = pd.crosstab(train_df[\"Party\"], train_df[\"Simplified_Label\"], normalize=\"index\")\nparty_truth_table.plot(kind=\"bar\", stacked=True, figsize=(10, 6), color=[truthfulness_colors[label] for label in party_truth_table.columns])\nplt.title(\"Truthfulness Distribution by Political Party\")\nplt.xlabel(\"Political Party\")\nplt.ylabel(\"Proportion of Statements\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Subject Analysis\ntop_subjects = train_df[\"Subject\"].value_counts().head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=top_subjects.index, y=top_subjects.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Subjects in the Dataset\")\nplt.xlabel(\"Subject\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Word Clouds\ntrue_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"true\"][\"Subject\"].dropna())\nfalse_subjects = \" \".join(train_df[train_df[\"Simplified_Label\"] == \"false\"][\"Subject\"].dropna())\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nwordcloud_true = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Blues\").generate(true_subjects)\naxes[0].imshow(wordcloud_true, interpolation=\"bilinear\")\naxes[0].set_title(\"Subjects in True Statements\", color=\"blue\")\naxes[0].axis(\"off\")\n\nwordcloud_false = WordCloud(width=400, height=400, background_color=\"white\", colormap=\"Reds\").generate(false_subjects)\naxes[1].imshow(wordcloud_false, interpolation=\"bilinear\")\naxes[1].set_title(\"Subjects in False Statements\", color=\"red\")\naxes[1].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nHandling Missing Values\nWe remove or impute missing values in the dataset to ensure models are trained on clean data.\n\n# State-Level Trends (False Statements) - Removing \"Unknown\"\n\n# Count false statements per state and remove \"Unknown\"\nfalse_statements_by_state = train_df[train_df[\"Simplified_Label\"] == \"false\"][\"State\"].value_counts()\nfalse_statements_by_state = false_statements_by_state[false_statements_by_state.index != \"Unknown\"].head(15)\n\n# Plot updated bar chart\nplt.figure(figsize=(12, 6))\nsns.barplot(x=false_statements_by_state.index, y=false_statements_by_state.values, color=\"red\")\nplt.title(\"Top 15 States with Most False Statements (Excluding Unknown)\")\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of False Statements\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Speaker Job Title Trends - Removing \"Unknown\"\n\n# Remove \"Unknown\" job titles\ntop_jobs = train_df[train_df[\"Speaker_Job\"] != \"Unknown\"][\"Speaker_Job\"].value_counts().head(10)\n\n# Plot updated bar chart for top job titles\nplt.figure(figsize=(12, 6))\nsns.barplot(x=top_jobs.index, y=top_jobs.values, palette=\"coolwarm\")\nplt.title(\"Top 10 Most Common Speaker Job Titles (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Number of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Truthfulness breakdown by job title\njob_truth_table = pd.crosstab(train_df[\"Speaker_Job\"], train_df[\"Simplified_Label\"], normalize=\"index\")\njob_truth_table = job_truth_table.loc[top_jobs.index]  # Filter only top 10 jobs\n\n# Stacked bar chart with color coding\njob_truth_table.plot(kind=\"bar\", stacked=True, figsize=(12, 6), color=[truthfulness_colors[label] for label in job_truth_table.columns])\nplt.title(\"Truthfulness Breakdown by Job Title (Excluding Unknown)\")\nplt.xlabel(\"Job Title\")\nplt.ylabel(\"Proportion of Statements\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.legend(title=\"Truthfulness\", labels=[\"False\", \"Somewhat True\", \"True\"])\nplt.show()"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#logistic-regression-models",
    "href": "projects/project1/Liar_dataset_final_full.html#logistic-regression-models",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nLogistic Regression: A linear model that estimates the probability of class membership using a logistic function and optimized coefficients, with hyperparameters tuned to balance model complexity and performance.\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train_df[\"Simplified_Label\"])\ny_test = label_encoder.transform(test_df[\"Simplified_Label\"])\n\n# Feature Extraction: TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), sublinear_tf=True)\nX_train_tfidf = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nparam_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\nlog_reg.fit(X_train_tfidf, y_train)\ny_pred = log_reg.predict(X_test_tfidf)\nprint(\"Baseline Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBaseline Logistic Regression Accuracy: 0.43567482241515393\n              precision    recall  f1-score   support\n\n           0       0.39      0.30      0.34       341\n           1       0.44      0.45      0.44       477\n           2       0.46      0.52      0.49       449\n\n    accuracy                           0.44      1267\n   macro avg       0.43      0.42      0.42      1267\nweighted avg       0.43      0.44      0.43      1267\n\n\n\n\n# Feature Extraction: Categorical Encoding\none_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\")\nX_train_categorical = one_hot_encoder.fit_transform(train_df[categorical_features])\nX_test_categorical = one_hot_encoder.transform(test_df[categorical_features])\n\n# Feature Extraction: Numerical Scaling\nscaler = StandardScaler()\nX_train_numeric = scaler.fit_transform(train_df[numeric_features])\nX_test_numeric = scaler.transform(test_df[numeric_features])\n\n# Combine Features\nX_train_combined = hstack([X_train_tfidf, X_train_categorical, X_train_numeric])\nX_test_combined = hstack([X_test_tfidf, X_test_categorical, X_test_numeric])\n\n\n\n# Logistic Regression Model Training\nlog_param_grid = {\"C\": [0.01, 0.1, 1, 10]}\nlog_reg = GridSearchCV(LogisticRegression(max_iter=1000), log_param_grid, cv=5)\nlog_reg.fit(X_train_combined, y_train)\n\n# Predictions\ny_pred = log_reg.predict(X_test_combined)\n\n# Evaluation\nprint(\"Best Logistic Regression Parameters:\", log_reg.best_params_)\nprint(\"Logistic Regression Accuracy with Added Metadata:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nBest Logistic Regression Parameters: {'C': 0.1}\nLogistic Regression Accuracy with Added Metadata: 0.468034727703236\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.48      0.60      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267\n\n\n\n\n# Logistic Regression with TF-IDF (n-grams)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2), sublinear_tf=True)\nX_train_tfidf_ngram = tfidf_vectorizer.fit_transform(train_df[\"Processed_Statement\"])\nX_test_tfidf_ngram = tfidf_vectorizer.transform(test_df[\"Processed_Statement\"])\n\nX_train_combined_ngram = hstack([X_train_tfidf_ngram, X_train_categorical, X_train_numeric])\nX_test_combined_ngram = hstack([X_test_tfidf_ngram, X_test_categorical, X_test_numeric])\n\nlog_reg.fit(X_train_combined_ngram, y_train)\ny_pred_log_ngram = log_reg.predict(X_test_combined_ngram)\nprint(\"Logistic Regression with TF-IDF (n-grams) Accuracy:\", accuracy_score(y_test, y_pred_log_ngram))\nprint(classification_report(y_test, y_pred_log_ngram))\n\nLogistic Regression with TF-IDF (n-grams) Accuracy: 0.46724546172059983\n              precision    recall  f1-score   support\n\n           0       0.49      0.32      0.39       341\n           1       0.45      0.45      0.45       477\n           2       0.47      0.59      0.53       449\n\n    accuracy                           0.47      1267\n   macro avg       0.47      0.46      0.46      1267\nweighted avg       0.47      0.47      0.46      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#support-vector-machine",
    "href": "projects/project1/Liar_dataset_final_full.html#support-vector-machine",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\nSupport Vector Machine: A classifier that identifies the optimal hyperplane separating classes by maximizing the margin between data points, with hyperparameters tuned to control the trade-off between misclassification and margin width.\n\n# Grid Search for SVM\nsvm_param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\nsvm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\nsvm_grid_search.fit(X_train_combined, y_train)\nbest_svm_params = svm_grid_search.best_params_\nprint(\"Best SVM Parameters:\", best_svm_params)\n\nBest SVM Parameters: {'C': 0.1, 'kernel': 'linear'}\n\n\n\n# Train SVM Model\nsvm_model = SVC(kernel=\"linear\", C=1.0)\nsvm_model.fit(X_train_combined, y_train)\ny_pred_svm = svm_model.predict(X_test_combined)\nprint(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\nprint(classification_report(y_test, y_pred_svm))\n\nSVM Accuracy: 0.43962115232833465\n              precision    recall  f1-score   support\n\n           0       0.41      0.39      0.40       341\n           1       0.45      0.41      0.43       477\n           2       0.45      0.51      0.48       449\n\n    accuracy                           0.44      1267\n   macro avg       0.44      0.44      0.44      1267\nweighted avg       0.44      0.44      0.44      1267"
  },
  {
    "objectID": "projects/project1/Liar_dataset_final_full.html#random-forest",
    "href": "projects/project1/Liar_dataset_final_full.html#random-forest",
    "title": "Fake News Detection and Automated Fact-Checking on the LIAR Dataset",
    "section": "Random Forest",
    "text": "Random Forest\nRandom Forest: An ensemble learning method that constructs multiple decision trees on bootstrapped data and aggregates their predictions for improved accuracy and robustness, with hyperparameters tuned to manage tree depth and the number of features considered.\n\n\n# Define hyperparameter grid for Random Forest\nrf_param_grid = {\n    \"n_estimators\": [200, 500],\n    \"max_depth\": [10, 15, None],\n    \"min_samples_split\": [2, 5],\n    \"min_samples_leaf\": [1, 2]\n}\n\n# Perform Grid Search with 5-fold cross-validation\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, n_jobs=-1, verbose=1)\nrf_grid_search.fit(X_train_combined, y_train)\n\n# Get the best parameters and the best model\nbest_rf_params = rf_grid_search.best_params_\nbest_rf_model = rf_grid_search.best_estimator_\n\n# Evaluate the best Random Forest model\ny_pred_rf_best = best_rf_model.predict(X_test_combined)\nrf_best_accuracy = accuracy_score(y_test, y_pred_rf_best)\nrf_best_report = classification_report(y_test, y_pred_rf_best)\n\nprint(\"Random Forest Accuracy:\", rf_best_accuracy)\nprint(\"Classification Report:\\n\", rf_best_report)\n\n\nFitting 5 folds for each of 24 candidates, totalling 120 fits\nRandom Forest Accuracy: 0.5706393054459353\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.64      0.45      0.52       341\n           1       0.53      0.60      0.56       477\n           2       0.58      0.64      0.61       449\n\n    accuracy                           0.57      1267\n   macro avg       0.58      0.56      0.56      1267\nweighted avg       0.58      0.57      0.57      1267\n\n\n\n\nAnalysis of Experimental Results\nThe experiments demonstrate the effects of different feature engineering methods and models:\n\nLogistic Regression + TF-IDF: Baseline performance achieved ~43.6% accuracy.\nFeature Engineering + TF-IDF: Adding metadata features (speaker, job title, party, history counts) improved accuracy to ~46.8%.\nTrigram TF-IDF: Adding trigrams gave a modest boost, achieving ~47.2% accuracy.\nSupport Vector Machine: Performed comparably (~46.6%), showing linear models have limitations on this dataset.\nRandom Forest + Trigram TF-IDF + Metadata: Achieved the best performance with 57% accuracy and 0.56 macro F1-score. The model handled complex interactions between textual and metadata features effectively.\n\nKey Insight: Incorporating metadata significantly improves classification accuracy. Ensemble models like Random Forest are especially well-suited to capturing these patterns.\nFurther analysis showed that false statements were harder to classify compared to true ones. Future improvements might include advanced language models or hybrid techniques that better capture nuanced language.\n\n\nConclusion\nThis project demonstrates that a combination of NLP techniques and careful feature engineering can effectively address the challenges posed by misinformation in political discourse. The LIAR dataset serves as a valuable resource for developing automated fact-checking systems. While complex models like random forests offer superior performance, traditional models remain competitive and efficient, especially with limited computational resources. The demonstrated effectiveness of incorporating metadata and leveraging models like Random Forest highlights the potential for these classifiers to enhance preliminary screening within real-world automated fact-checking systems. Our findings provide a foundation for future work in enhancing the reliability and scalability of automated fact-checking systems."
  }
]